{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e894931",
   "metadata": {},
   "source": [
    "# Notions fondamentales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd01477",
   "metadata": {},
   "source": [
    "Dans le contexte des réseaux de neurones traitant des données bidimensionnelles (2D), comme les images, les données d'entrée sont habituellement représentées sous la forme d'un tenseur à quatre dimensions. Ce tenseur est noté $(N_{\\text{total}}, C, H, W)$, où $N_{\\text{total}}$ représente le nombre total d'images, $C$ le nombre de canaux (par exemple, 3 pour une image RGB), et $H$ et $W$ désignent respectivement la hauteur et la largeur de chaque image.\n",
    "\n",
    "Un échantillon unique dans ce contexte correspond à une donnée 2D, c’est-à-dire une image, qui a pour dimension $(C, H, W)$.\n",
    "\n",
    "Pour améliorer l'efficacité et la performance des réseaux de neurones, il est courant de regrouper les données en 'batches'. Un batch est un sous-ensemble du jeu de données complet et est de dimension $(N, C, H, W)$, où $N$ indique la taille du batch, c’est-à-dire le nombre d'échantillons traités ensemble lors d'une seule étape de l'apprentissage.\n",
    "\n",
    "Nous utiliserons ces notations et concepts tout au long de notre explication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a5558",
   "metadata": {},
   "source": [
    "# Définition et calcul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978055ff",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09063b4",
   "metadata": {},
   "source": [
    "La normalisation des données joue un rôle crucial dans l'apprentissage des réseaux de neurones profonds. Bien que la normalisation à l'entrée soit essentielle, elle s'avère souvent insuffisante en raison d'un phénomène connu sous le nom de 'gradient evanescent'. Ce dernier se manifeste par une réduction progressive de l'amplitude du gradient lors de la rétropropagation, particulièrement due aux faibles valeurs de sortie des couches d'activation, comme les fonctions sigmoid ou tanh. Dans les réseaux comportant de nombreuses couches, la multiplication répétée de ces petits termes peut entraîner la diminution significative du gradient.\n",
    "\n",
    "Pour contrer ce problème, une renormalisation des données à travers les différentes couches du réseau s'avère nécessaire. De plus, un autre défi, connu sous le nom de 'décalage covariant interne', survient. Ce phénomène résulte de la modification de la distribution des activations à chaque couche, due aux transformations non linéaires appliquées par le réseau. Autrement dit, même si les données en entrée présentent une moyenne de 0 et une variance de 1, ces caractéristiques se déforment au fur et à mesure qu'elles traversent les différentes couches du réseau. Des techniques telles que Batch Normalization ont été développées pour répondre à ces enjeux.\n",
    "\n",
    "Bien que les objectifs de Batch Normalization et de Instance Normalization, telle que proposée dans le papier en question, soient différents (la première visant à renormaliser les données pour éviter les problèmes précédemment décrits, tandis que la seconde a pour but de neutraliser le contrast et la variation des couleurs au sein d'une image), une étude de Batch Normalization reste pertinente. En effet, dans les deux cas, il s'agit d'une forme de normalisation des données, bien que leurs applications et finalités soient distinctes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5394e1b",
   "metadata": {},
   "source": [
    "La formule générale pour la normalisation dans les réseaux de neurones est la suivante :\n",
    "\n",
    "$$ Normalisation(x)=\\gamma (\\frac{x-\\mu(x)}{\\sigma(x)})+\\beta, $$\n",
    "\n",
    "où $x$ est une image, $\\gamma, \\beta$ sont des paramètres affines extraits des données (les paramètres d'échelle et de décalage resp.). \n",
    "\n",
    "La différence dans les formules entre Instance Normalization et Batch Normalization vient dans la définition de $\\mu(x), \\sigma(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d9d36",
   "metadata": {},
   "source": [
    "## Batch normalization (BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c85806",
   "metadata": {},
   "source": [
    "Batch Normalization consiste à normaliser les activations d'un réseau de neurones pour chaque lot (ou batch) de données traitées. Concrètement, au lieu de normaliser une seule image (ou échantillon), cette technique ajuste les activations de toutes les images dans un batch en fonction de la moyenne et de la variance calculées sur l'ensemble du batch, et ce, pour chaque canal de manière indépendante, comme illustré sur la figure ci-dessous. Ainsi, pour chaque canal, les activations sont d'abord centrées autour de zéro (en soustrayant la moyenne du batch) puis mises à l'échelle (en divisant par l'écart-type du batch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577bd9ce",
   "metadata": {},
   "source": [
    "Les expressios de la moyenne et de l'écart-type de BN sont les suivantes : \n",
    "\n",
    "$$\\mu_{batch}(x) = \\frac{1}{NHW}\\sum_{n=1}^{N}\\sum_{h=1}^{H}\\sum_{w=1}^{W}x_{nchw},$$\n",
    "\n",
    "$$\n",
    "\\sigma_{batch}(x) = \\sqrt{\\frac{1}{NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} \\left(x_{nchw} - \\mu_{batch}(x)\\right)^2+\\epsilon}, \n",
    "$$\n",
    "\n",
    "où $x_{nchw}$ est une image de batch $n$ de taille $h$ x $w$ et de canal $c$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29f1ea",
   "metadata": {},
   "source": [
    "![BN](/Users/ako/Desktop/HDDL_AdaIN_git/HDDL_AdaIN/resources/images/BN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd70f1f",
   "metadata": {},
   "source": [
    "## Instance normalization (IN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ff840",
   "metadata": {},
   "source": [
    "L'Instance Normalization, ou normalisation d'instance, consiste à normaliser chaque échantillon individuellement en fonction de sa propre moyenne et variance pour chaque canal. Concrètement, cela signifie que pour chaque image dans un batch, la normalisation est effectuée séparément, canal par canal. Cette approche est particulièrement utile dans les tâches où l'indépendance entre les échantillons est cruciale, comme dans le transfert de style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b967c54",
   "metadata": {},
   "source": [
    "Les expressios de la moyenne et de l'écart-type de IN sont les suivantes : \n",
    "\n",
    "$$\\mu_{image}(x) = \\frac{1}{HW}\\sum_{h=1}^{H}\\sum_{w=1}^{W}x_{chw},$$\n",
    "\n",
    "$$\n",
    "\\sigma_{image}(x) = \\sqrt{\\frac{1}{HW} \\sum_{h=1}^{H} \\sum_{w=1}^{W} \\left(x_{chw} - \\mu_{image}(x)\\right)^2+\\epsilon}, \n",
    "$$\n",
    "\n",
    "où $x_{chw}$ est une image de taille $h$ x $w$ et de canal $c$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ac38b",
   "metadata": {},
   "source": [
    "![IN](/Users/ako/Desktop/HDDL_AdaIN_git/HDDL_AdaIN/resources/images/IN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd0d45",
   "metadata": {},
   "source": [
    "# Différences dans l'application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6642d650",
   "metadata": {},
   "source": [
    "La normalisation par batchs suppose que les images dans un batch donné sont représentatives de l'ensemble du jeu de données. Cette hypothèse peut ne pas toujours être vraie, surtout pour les petits batchs.\n",
    "\n",
    "D'autre part, la normalisation d'instance (Instance Normalization) opère sous l'hypothèse que chaque image possède son propre style. En conséquence, elle normalise les données en fonction de la moyenne et de la variance de chaque échantillon individuel, et ce, pour chaque canal. Cette méthode est particulièrement adaptée à des applications telles que le transfert de style, où le style unique de chaque image est un facteur clé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87671087",
   "metadata": {},
   "source": [
    "Pour la Batch Normalization (BN), les couches utilisent des statistiques de batch (moyenne et variance) durant l'entraînement, et des statistiques de population (moyenne et variance de l'ensemble de données d'apprentissage) durant le test. Ceci est dû à l'hypothèse de BN selon laquelle un batch est représentatif de l'ensemble du jeu de données d'apprentissage, ce qui n'est pas nécessairement le cas dans un contexte de test. Pour améliorer la robustesse, on remplace donc les statistiques du batch par celles de la population durant l'inférence. En revanche, la normalisation d'instance (IN) ne repose pas sur une telle hypothèse. Ainsi, les statistiques restent inchangées entre l'apprentissage et le test. En résumé, pour la BN, les statistiques utilisées durant l'inférence sont fixes (celles tirées de l'ensemble d'entraînement), tandis que pour l'IN, les statistiques sont calculées aussi bien durant l'apprentissage que lors du test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d28f87",
   "metadata": {},
   "source": [
    "![BN vs IN](/resources/images/BN_vs_IN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a55f3",
   "metadata": {},
   "source": [
    "D'après les graphiques présentés ci-dessus, on observe que l'Instance Normalization (IN) contribue significativement à la réduction de la perte de style lorsqu'on entraîne le réseau avec des images originales et celles normalisées en termes de contraste. Toutefois, l'amélioration apportée par l'IN semble moins marquée pour les images préalablement normalisées en fonction de leur style. Cette observation s'aligne avec les explications précédentes : l'un des principaux atouts de l'IN réside dans son adaptation aux cas où chaque image présente un style individuel distinct. Or, dans cette situation, le style intrinsèque des images d'entrée a été normalisé, ce qui réduit l'effet et l'utilité de l'IN. Par conséquent, les améliorations apportées par l'IN dans ce contexte spécifique apparaissent relativement limitées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da06322",
   "metadata": {},
   "source": [
    "## Principe du transfert de style :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ae668",
   "metadata": {},
   "source": [
    " Le transfert de style est un processus permettant de transferer le style d'une image à une image dont l'on souhaite garder le contenue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0fb61b",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae0cc2",
   "metadata": {},
   "source": [
    "On considére deux images de même taille : une correspondant à l'image contenue notée $\\textbf{ImgC}$ et une correspondant à l'image style notée $\\textbf{ImgS}$.  Le transfert de style repose sur un mécanisme qui comprend un encodeur, ADAIN et un décodeur. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c98a32",
   "metadata": {},
   "source": [
    "## 1. Encodeur "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf1043",
   "metadata": {},
   "source": [
    "L'encodeur va permettre d'extraire les $\\textbf{features}$ de l'image de contenue $ImgC$  d'une part et les $\\textbf{features}$ de l'image de style $ImgS$ d'autre part, que l'on note respectivement $\\text{f(ImgC)}$ et $\\text{f(ImgS)}$.  Cette encodeur est composé de 13 couches de réseaux de neuronnes dont 10 couches de convolutions et 3 couche de Maxpoolings. Considérons une image d'entrée de dimension (h,w,c), alors on décrit l'encodeur par le tableau suivant : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c02a55",
   "metadata": {},
   "source": [
    "![Texte alternatif de l'image](Encodeur.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e0cf9",
   "metadata": {},
   "source": [
    "où h, w et c sont respectivement la hauteur, la largeur et le nombre de canaux de l'image en entrée. De plus, les dimensions des cartes fonctionnelles intermédiaires sont données par les formules suivantes:   $w_1$=$[\\frac{w}{2}-1]+1$ , $h_1$= $[\\frac{h}{2}-1]+1$ , $w_2$=$[\\frac{w_1}{2}-1]+1$, $h_2$=$[\\frac{h_1}{2}-1]+1$, $w_3$=$[\\frac{w_2}{2}-1]+1$ et $h_3$=$[\\frac{h_2}{2}-1]+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ccbebb",
   "metadata": {},
   "source": [
    "$\\textbf{Remarque} :$\n",
    "\n",
    "L’encodeur transforme l’entrée en une représentation dans un espace de dimension plus faible appelé espace latent. L’encodeur compresse donc l’entrée dans une représentation moins coûteuse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098cd09b",
   "metadata": {},
   "source": [
    "## 2. ADAIN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96192504",
   "metadata": {},
   "source": [
    "Après l’encodeur, nous connaissons la taille de la carte de fonctionnalités : pour une image d’entrée (w,h,C), nous obtenons une carte de fonctionnalités de dimension ($w_3$, $h_3$, 512) pour l'image de style et de contenue.  L'instance de normalisation adaptative opère en normalisant la carte de fonctionnalités de l'image contenue, représentée par $x = \\text{f(ImgC)}$, en se basant sur les statistiques obtenues à partir de la carte de fonctionnalités de l'image style, notée $y = \\text{f(ImgS)}$. Nous avons illustrer le mécanisme par le schéma suivant :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0951b87",
   "metadata": {},
   "source": [
    "![Texte alternatif de l'image](ADAIN.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8aaa8",
   "metadata": {},
   "source": [
    "La première étape de ce processus implique le calcul de la moyenne des valeurs des pixels de chaque canal sur toute la largeur et la hauteur du tenseur d'entrée, ainsi que l'écart-type des valeurs des pixels de chaque canal sur l'ensemble de ces dimensions. En résultat, nous obtenons des vecteurs moyennes $\\mu_c$ et $\\mu_s$ pour chaque canal de la carte de fonctionnalités de contenu et de style, accompagnés de leurs écart-types respectifs notés  $\\sigma_c$ et $\\sigma_s$, tous ces vecteurs étant de dimension 512."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9ae2e",
   "metadata": {},
   "source": [
    "La seconde étape consiste à normalisé chacun des pixels de la carte de fonctionnalité de contenues par la moyenne $\\mu_c^c$ et l'écart-type $\\sigma_c^c$ de tout valeurs des pixels qui appartiennent au même canal. Cette valeur va être alignée au valeur de la carte fonctionnalité de style en y multipliant l'écart-type $\\sigma_s^c$ de tout valeurs des pixels qui appartiennent au même canal dans la carte de fonctionnalité style et en y ajoutant la moyenne $\\mu_s^c$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182986ca",
   "metadata": {},
   "source": [
    "A la fin de ces étapes, nous obtenons donc la carte de fonctionalité cible que l'on note t= $ADAIN(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f20acc",
   "metadata": {},
   "source": [
    "## Extension de Instance normalisation, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c2e37",
   "metadata": {},
   "source": [
    "## 2. Décodeur "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54110a3b",
   "metadata": {},
   "source": [
    "A partir du $\\textbf{feature de cible}$, c'est-à-dire les features obtenues par la méthode ADAIN, le décodeur va essayer de reconstruire l'image contenue stylisé. Le décodeur est composé de 12 couches de réseaux de neuronnes dont 10 couches de convolutions et  de Upsampling.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37fc247",
   "metadata": {},
   "source": [
    "![Texte alternatif de l'image](decodeur.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7681461e",
   "metadata": {},
   "source": [
    "$\\textbf{Remarque} $ : \n",
    "    On remarque la présence de 3 couche UPSAMPLING permettant de redimensionner ou agrandir le tenseur d'entrée d’un facteur 2, ‘nearest’ indique une interpolation par le plus proche voisin. Cette méthode attribue la valeur du pixel d'entrée le plus proche au pixel correspondant en sortie, en faisant une méthode d'interpolation simple et rapide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c1490",
   "metadata": {},
   "source": [
    "#### Nombre total de paramétres à estimer :\n",
    "\n",
    "5 240 704 + 3 * c paramétres à estimer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ced48",
   "metadata": {},
   "source": [
    "## Entrainement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a20ca6",
   "metadata": {},
   "source": [
    "Le modéle est entrainé sur 80 000 images, on utilise l'optimiseur ADAM et le taille de batch est égale à 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14cd2d8",
   "metadata": {},
   "source": [
    "## Perte liée au contenue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424be17",
   "metadata": {},
   "source": [
    "   Cette perte permet de quantifier la perte liées au contenue en effet l’encodeur va permettre d’extraire les features des images de style $f(ImgS)$ et de contenue $f(ImgC)$. Ces features vont être transmise à la couche $ADAIN()$ qui aligne la moyenne et la variance des cartes de fonctionnalités de contenu sur celles de les cartes de caractéristiques de style, produisant les caractéristiques cibles que l'on note t= $ADAIN(f(ImgS), f(ImgC))$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82ddc90",
   "metadata": {},
   "source": [
    "![Texte alternatif de l'image](architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a25f36a",
   "metadata": {},
   "source": [
    " Ensuite cette carte de fonctionnalités va être reconstruite par un décodeur afin d’avoir une image ayant le contenue et style souhaité g(t) . Ensuite, on va extraire la carte fonctionnelle de cette image dont le contenue à été stylisé, on passe par un encodeur : on obtient les caractéristique de l’image reconstruite ayant le contenue et style souhaité f(g(t)). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81d678",
   "metadata": {},
   "source": [
    "$$ L_c = || f(g(t)) -t||_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048fc127",
   "metadata": {},
   "source": [
    "La perte du contenue correspond à la différence euclidienne entre les caractéristiques de fonctionnalité à la sortie de l’ADAIN() et celle extraite de l’image reconstruite ayant le contenue et style souhaité, dont on vérifie bien la compatibilité au niveau des dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fafa7ac",
   "metadata": {},
   "source": [
    "### PRÉ-entrainement ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a4712d",
   "metadata": {},
   "source": [
    "## Perte liée au style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9f96d9",
   "metadata": {},
   "source": [
    "Le transfert de style s'effectue en adaptant les statistiques de l'image de contenu pour correspondre à celles de l'image de style, en se concentrant particulièrement sur la moyenne et l'écart-type. Ainsi, il est logique d'utiliser une perte de style qui vise à minimiser la différence entre ces statistiques pour la sortie du réseau et l'image de style de référence. La formule de perte de style présentée dans le papier est la suivante :\n",
    "\n",
    "$$\n",
    "L_s = \\sum_{i=1}^{L} \\left\\| \\mu(\\phi_i(g(t))) - \\mu(\\phi_i(s)) \\right\\|^2 + \\sum_{i=1}^{L} \\left\\| \\sigma(\\phi_i(g(t))) - \\sigma(\\phi_i(s)) \\right\\|^2,\n",
    "$$\n",
    "où $L$ est le nombre total des couches de VGG-19, $\\mu$ et $\\sigma$ sont respectivement la moyenne et l'écart-type des activations d'une couche de VGG-19, $s$ est l'image de style de référence et $g(t)$ est l'image stylisée, i.e. la sortie finale du réseau. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c2e18",
   "metadata": {},
   "source": [
    "Remarque : Dans leurs expériences, les auteurs ont choisi d'utiliser quatre couches spécifiques du réseau VGG-19.\n",
    "\n",
    "Le papier mentionne également l'usage courant de la perte de Gram, une autre méthode fréquemment employée dans le transfert de style. Cependant, bien que les résultats obtenus avec cette méthode soient similaires, les auteurs soulignent qu'il est conceptuellement plus approprié d'adopter la perte de style décrite précédemment. Cette approche est en effet plus cohérente avec l'objectif de minimiser les différences entre les statistiques clés (moyenne et écart-type) des images de style et de contenu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d5aa6",
   "metadata": {},
   "source": [
    "## Limitation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
